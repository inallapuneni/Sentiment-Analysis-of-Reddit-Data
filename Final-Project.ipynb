{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[' Timestamp', 'AuthorName', 'DiscussionTitle', 'DiscussionBody']]\n",
    "\n",
    "# clean the Timestamp column\n",
    "df[' Timestamp'] = df[' Timestamp'].str.replace(\"'\", \"\")\n",
    "\n",
    "# clean the Timestamp column\n",
    "df[' Timestamp'] = pd.to_datetime(df[' Timestamp'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "# print the first few rows to check\n",
    "print(df.head())\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "\n",
    "# load the dataset into a pandas dataframe\n",
    "\n",
    "\n",
    "# clean the text data\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['cleaned_text'] = df['DiscussionBody'].apply(lambda x: ' '.join([word for word in x.lower().split() if word not in stop_words]))\n",
    "\n",
    "# apply sentiment analysis using TextBlob\n",
    "df['sentiment'] = df['cleaned_text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "# print the overall sentiment score of each DiscussionBody\n",
    "print(df[['DiscussionBody', 'sentiment']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install vaderSentiment\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# assume that your recipe data is loaded into a pandas dataframe called `recipe_df`\n",
    "df.to_csv('recipe_sentiment.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import csv\n",
    "import urllib.request\n",
    "\n",
    "# Preprocess text (username and link placeholders)\n",
    "# def preprocess(text):\n",
    "#     new_text = []\n",
    " \n",
    " \n",
    "#     for t in text.split(\" \"):\n",
    "#         t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "#         t = 'http' if t.startswith('http') else t\n",
    "#         new_text.append(t)\n",
    "#     return \" \".join(new_text)\n",
    "\n",
    "# Tasks:\n",
    "# emoji, emotion, hate, irony, offensive, sentiment\n",
    "# stance/abortion, stance/atheism, stance/climate, stance/feminist, stance/hillary\n",
    "\n",
    "task='sentiment'\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "# download label mapping\n",
    "labels=[]\n",
    "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
    "with urllib.request.urlopen(mapping_link) as f:\n",
    "    html = f.read().decode('utf-8').split(\"\\n\")\n",
    "    csvreader = csv.reader(html, delimiter='\\t')\n",
    "labels = [row[1] for row in csvreader if len(row) > 1]\n",
    "\n",
    "# PT\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "model.save_pretrained(MODEL)\n",
    "df['positive'] = 0.0\n",
    "df['neutral'] = 0.0\n",
    "df['negative'] = 0.0\n",
    "\n",
    "for ind in df.index:\n",
    "    print(df['DiscussionBody'][ind])\n",
    "    text = df['DiscussionBody'][ind]\n",
    "# text = \"I dont like cars, they are accident prone\"\n",
    "    # text = preprocess(text)\n",
    "    encoded_input = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512, add_special_tokens = True)\n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "\n",
    "\n",
    "    ranking = np.argsort(scores)\n",
    "    ranking = ranking[::-1]\n",
    "    for i in range(scores.shape[0]):\n",
    "      l = labels[ranking[i]]\n",
    "      s = scores[ranking[i]]\n",
    "      print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n",
    "      df[l][ind] = np.round(float(s), 4)\n",
    "\n",
    "    # if ind > 10:\n",
    "    #   break\n",
    "\n",
    "\n",
    "# df.head(10)\n",
    "df_result = df.groupby(by = 'DiscussionTitle', as_index=False).positive.agg('mean')\n",
    "df_result\n",
    "df_result_final = df_result.sort_values(by='positive', ascending=False)\n",
    "df_result_final.head(25)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
